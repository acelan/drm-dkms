From 2db31c710a8e01088f8771fbe6081804dd1999e5 Mon Sep 17 00:00:00 2001
From: Timo Aaltonen <timo.aaltonen@canonical.com>
Date: Tue, 12 Sep 2017 12:19:13 +0300
Subject: [PATCH 2/3] Revert "drm/i915: Use insert_page for pwrite_fast"

This reverts commit 4f1959ee33c0a0a1bb22a911a162744e30ef6d97.
---
 drivers/gpu/drm/i915/i915_gem.c | 95 ++++++++++-------------------------------
 1 file changed, 22 insertions(+), 73 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
index 4e09222..febf9a1 100644
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -59,24 +59,6 @@ static bool cpu_write_needs_clflush(struct drm_i915_gem_object *obj)
 	return obj->pin_display;
 }
 
-static int
-insert_mappable_node(struct drm_i915_private *i915,
-                     struct drm_mm_node *node, u32 size)
-{
-	memset(node, 0, sizeof(*node));
-	return drm_mm_insert_node_in_range_generic(&i915->ggtt.base.mm, node,
-						   size, 0, 0, 0,
-						   i915->ggtt.mappable_end,
-						   DRM_MM_SEARCH_DEFAULT,
-						   DRM_MM_CREATE_DEFAULT);
-}
-
-static void
-remove_mappable_node(struct drm_mm_node *node)
-{
-	drm_mm_remove_node(node);
-}
-
 /* some bookkeeping */
 static void i915_gem_info_add_obj(struct drm_i915_private *dev_priv,
 				  size_t size)
@@ -970,42 +952,24 @@ fast_user_write(struct io_mapping *mapping,
  * @file: drm file pointer
  */
 static int
-i915_gem_gtt_pwrite_fast(struct drm_i915_private *i915,
+i915_gem_gtt_pwrite_fast(struct drm_device *dev,
 			 struct drm_i915_gem_object *obj,
 			 struct drm_i915_gem_pwrite *args,
 			 struct drm_file *file)
 {
-	struct i915_ggtt *ggtt = &i915->ggtt;
+	struct drm_i915_private *dev_priv = to_i915(dev);
+	struct i915_ggtt *ggtt = &dev_priv->ggtt;
 	struct i915_vma *vma;
-	struct drm_mm_node node;
-	uint64_t remain, offset;
+	ssize_t remain;
+	loff_t offset, page_base;
 	char __user *user_data;
-	int ret;
+	int page_offset, page_length, ret;
 
 	vma = i915_gem_object_ggtt_pin(obj, NULL, 0, 0,
 				       PIN_MAPPABLE | PIN_NONBLOCK);
-	if (!IS_ERR(vma)) {
-		node.start = i915_ggtt_offset(vma);
-		node.allocated = false;
-		ret = i915_vma_put_fence(vma);
-		if (ret) {
-			i915_vma_unpin(vma);
-			vma = ERR_PTR(ret);
-		}
-	}
-	if (IS_ERR(vma)) {
-		ret = insert_mappable_node(i915, &node, PAGE_SIZE);
-		if (ret)
-			goto out;
-
-		ret = i915_gem_object_get_pages(obj);
-		if (ret) {
-			remove_mappable_node(&node);
-			goto out;
-		}
 
-		i915_gem_object_pin_pages(obj);
-	}
+	if (IS_ERR(vma))
+		goto out;
 
 	ret = i915_gem_object_set_to_gtt_domain(obj, true);
 	if (ret)
@@ -1015,32 +979,26 @@ i915_gem_gtt_pwrite_fast(struct drm_i915_private *i915,
 	if (ret)
 		goto out_unpin;
 
-	intel_fb_obj_invalidate(obj, ORIGIN_CPU);
-	obj->dirty = true;
-
 	user_data = to_user_ptr(args->data_ptr);
-	offset = args->offset;
 	remain = args->size;
-	while (remain) {
+
+	offset = vma->node.start + args->offset;
+
+	intel_fb_obj_invalidate(obj, ORIGIN_CPU);
+
+	while (remain > 0) {
 		/* Operation in this page
 		 *
 		 * page_base = page offset within aperture
 		 * page_offset = offset within page
 		 * page_length = bytes to copy for this page
 		 */
-		u32 page_base = node.start;
-		unsigned page_offset = offset_in_page(offset);
-		unsigned page_length = PAGE_SIZE - page_offset;
-		page_length = remain < page_length ? remain : page_length;
-		if (node.allocated) {
-			wmb(); /* flush the write before we modify the GGTT */
-			ggtt->base.insert_page(&ggtt->base,
-					       i915_gem_object_get_dma_address(obj, offset >> PAGE_SHIFT),
-					       node.start, I915_CACHE_NONE, 0);
-			wmb(); /* flush modifications to the GGTT (insert_page) */
-		} else {
-			page_base += offset & PAGE_MASK;
-		}
+		page_base = offset & PAGE_MASK;
+		page_offset = offset_in_page(offset);
+		page_length = remain;
+		if ((page_offset + remain) > PAGE_SIZE)
+			page_length = PAGE_SIZE - page_offset;
+
 		/* If we get a fault while copying data, then (presumably) our
 		 * source page isn't available.  Return the error and we'll
 		 * retry in the slow path.
@@ -1059,16 +1017,7 @@ i915_gem_gtt_pwrite_fast(struct drm_i915_private *i915,
 out_flush:
 	intel_fb_obj_flush(obj, false, ORIGIN_CPU);
 out_unpin:
-	if (node.allocated) {
-		wmb();
-		ggtt->base.clear_range(&ggtt->base,
-				       node.start, node.size,
-				       true);
-		i915_gem_object_unpin_pages(obj);
-		remove_mappable_node(&node);
-	} else {
-		i915_vma_unpin(vma);
-	}
+	i915_vma_unpin(vma);
 out:
 	return ret;
 }
@@ -1312,7 +1261,7 @@ i915_gem_pwrite_ioctl(struct drm_device *dev, void *data,
 	if (obj->tiling_mode == I915_TILING_NONE &&
 	    obj->base.write_domain != I915_GEM_DOMAIN_CPU &&
 	    cpu_write_needs_clflush(obj)) {
-		ret = i915_gem_gtt_pwrite_fast(dev_priv, obj, args, file);
+		ret = i915_gem_gtt_pwrite_fast(dev, obj, args, file);
 		/* Note that the gtt paths might fail with non-page-backed user
 		 * pointers (e.g. gtt mappings when moving data between
 		 * textures). Fallback to the shmem path in that case. */
-- 
2.7.4

